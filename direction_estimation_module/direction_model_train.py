# -*- coding: utf-8 -*-
"""direction_model_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HH-M9Toi875HXzEuCnjLL4rrULiVgAwF
"""

#!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg
#!pip install pyaudio

import tensorflow as tf
import tensorflow.keras.layers as L
import tensorflow.keras.models as M
import tensorflow.keras.optimizers as O

import wave
import pandas as pd
import pyaudio
import os
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import wave

#audio = wave.open('/content/drive/My Drive/voice_model_train/direction_recordings/100.wav')
audio = wave.open('direction_recordings/100.wav')

audio.getnframes()

BATCH_SIZE = 8
SAMPLE_RATE = 16000
CHANNELS = 1
EPOCHS =10
RECORD_SECONDS = 3
BUFFER_SIZE = 256
FRAMES = audio.getnframes()
DIM = (int(FRAMES//BUFFER_SIZE),BUFFER_SIZE,1)

minmax = MinMaxScaler([0,1])

#df = pd.read_csv('/content/drive/My Drive/voice_model_train/voice_data_direction.csv')
df = pd.read_csv('voice_data_direction.csv')

print(df.label.value_counts())

df.columns

y_cols = df.columns[2:]
y_cols = list(y_cols)
print(y_cols)



p = pyaudio.PyAudio()

config = tf.compat.v1.ConfigProto(gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8))
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(session)


class Data_Generator(tf.keras.utils.Sequence):
    
    def __init__(self,batch_size,music_ids,dim,target=None,train=True,augment=False):
        self.batch_size = batch_size
        self.music_ids = music_ids
        self.augment = augment
        self.dim = dim
        self.target = target
        self.indices = range(len(self.music_ids))
        self.train = train
    
    def on_epoch_end(self):
        return self.indices
    
    def getdata(self, music_id_list):
        X = np.zeros((self.batch_size//4,4,*self.dim))
        X1 = np.zeros((4,*self.dim))
        counter = 1
        for l in range(self.batch_size//4):
            for i, m_id in enumerate(music_id_list):
                #audio = wave.open(f'/content/drive/My Drive/voice_model_train/direction_recordings/{m_id}','r')
                audio = wave.open(f'direction_recordings/{m_id}','r')
                frames = []
                for j in range(self.dim[0]):
                    au = audio.readframes(self.dim[1])
                    au = np.fromstring(au,np.int16)
                    frames.append(au)
                frames = np.array(frames,dtype=np.int16)
                frames = np.reshape(frames,(self.dim[0],self.dim[1],self.dim[2]))
                audio.close()              
                X1[counter-1,] = frames
                if counter%4 ==0:
                    X[l,] = np.array(X1,dtype=np.int16)
                    counter = 0
                counter+=1
        return X
        
    def __getitem__(self,index):
        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        
        music_id_list = [self.music_ids.values[k] for k in indices]
        X = self.getdata(music_id_list)
        #X = np.reshape(X,(self.batch_size,*self.dim,1))
        if self.train == True:
            y = [self.target.values[k] for k in indices[::4]]
            y = np.array(y).astype(np.int16)
            return X,y
        return X
    
    def __len__(self):
        return int(np.floor(len(self.indices)/self.batch_size))


def build_model_lstm():
        
    inp = L.Input(shape=(4,*DIM,),name='input')

    x = L.BatchNormalization()(inp)
    
    x = L.Bidirectional(L.ConvLSTM2D(32,3))(x)
    
    x = L.MaxPooling2D()(x)
    
    x = L.Dropout(0.2)(x)

    x = L.Flatten()(x)
    
    pred1 = L.Dense(8,activation = 'softmax',name = 'dense_f1')(x)
    
    model = M.Model(inputs=inp,outputs=pred1)
    
    model.compile(loss='categorical_crossentropy', optimizer=O.Adam(learning_rate=0.001),metrics=['acc'])
    
    return model

model = build_model_lstm()
model.summary()

def lrfn(epoch, lr):
    lr_max = 0.01
    lr_min = 0.00001
    lr_decay = 0.95
    epoch_sust  = 2
    if epoch % epoch_sust == 0:
        lr = lr_max * lr_decay**((epoch+1)/0.19) 
        return lr
    else:
        return lr


lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn,verbose=1)

counter = 1
a = []
lr = 0.001
for i in range(EPOCHS):
  lr = lrfn(i,lr)
  a.append(lr)
plt.plot(a)
plt.show()

print('\n',len(df.label.values),'\n')

train_gen = Data_Generator(BATCH_SIZE,
                           df.name,DIM,
                           target=df[y_cols],
                           train=True)

history = model.fit(train_gen,
                    epochs=EPOCHS,
                    steps_per_epoch=(len(df.label.values)//BATCH_SIZE))

#model.save('/content/drive/My Drive/voice_model_train/direction_model_lstm.h5')

model.save('direction_model_lstm_v2.h5')

test_gen = Data_Generator(BATCH_SIZE,
                           df.name,DIM,
                           target=df.label,
                           train=False)


pred = model.predict(train_gen,verbose=1)

print(model.evaluate(train_gen,verbose=0))


tf.compat.v1.keras.backend.clear_session()

#os.system("shutdown /s /t 1")
